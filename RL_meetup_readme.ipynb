{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-meetup-readme.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IALeMans/RL-meetup-Reinforcement_Learning/blob/master/RL_meetup_readme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "P09vqP06d88D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iUB2L_ROib4z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Apprentissage par renforcement - Reinforcement Learning (RL)\n"
      ]
    },
    {
      "metadata": {
        "id": "twD8CVZ9pF5d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## introduction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> ![Texte alternatif…]( https://cdn-images-1.medium.com/max/800/1*8OSHpISmR1l79yX4I234wg.jpeg)\n",
        "\n",
        "*source : https://medium.com/@shweta_bhatt/reinforcement-learning-101-e24b50e1d292*\n",
        "\n",
        "\n",
        "\n",
        " \n",
        " \n",
        "\n",
        "\n",
        "*   Supervised learning : Task Driven (Classification)\n",
        "\n",
        "*   Unsupervised learning : Data Driven (Clustering)\n",
        "\n",
        "*  Reinforcement learning : Interagir avec un environnement\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Texte alternatif…](https://cdn-images-1.medium.com/max/800/0*ME2TyTSp3fuK37e4.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5yRC2bwDn6od",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**GAME IA**\n",
        "\n",
        ">  ![Texte alternatif…](https://cdn-images-1.medium.com/max/800/1*D7JNcbvhP5UOR6_Ul-WJaw.gif)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mMD9_-It4eXP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL : principe\n",
        "\n",
        "![Texte alternatif…](https://s3.amazonaws.com/media-p.slid.es/uploads/ericmoura/images/1213308/rl.png)\n",
        "\n",
        "\n",
        "\n",
        "propriété de Markov ?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Essais-erreur + récomponse \n",
        "\n",
        "* recherche par essai-erreur\n",
        "* récompense à long terme \n",
        "\n",
        "sont les deux caractéristiques les plus importantes de\n",
        "l’apprentissage par renforcement.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# RL : compromis entre Exploration et Exploitation\n",
        "\n",
        "*  **Exploration**  : pour chercher une meilleure action \n",
        "*  **Exploitation** : pour obtenir une récompense connue\n",
        "\n",
        "\n",
        "L’un des défis de l’AR est le compromis à trouver entre exploration et exploitation. \n",
        "\n",
        "Pour obtenir beaucoup de récompense, un agent AR doit préférer des actions qu’il a essayées avant\n",
        "et trouvées être efficaces par les récompenses qu’elles ont entraîné. L’agent doit *exploiter* ce\n",
        "qu’il connaît déjà pour obtenir une récompense, mais il doit aussi *explorer* pour faire de\n",
        "meilleure actions dans le futur. Le dilemme est que ni l’exploration ni l’exploitation ne\n",
        "peuvent être poursuivie exclusivement sans faillir à la tâche."
      ]
    },
    {
      "metadata": {
        "id": "xOiohqMqDOCL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL : Formes de RL : passif et actif\n",
        "\n",
        " On distingue deux sortes d’apprentissage par renforcement : \n",
        " \n",
        "1. **passif** : Le plan à suivre est connue à l’avance. \n",
        "\n",
        "2. **actif** : L’agent doit aussi déterminer le plan optimal. \n",
        "\n",
        "\n",
        "\n",
        "## passif \n",
        "![Texte alternatif…](https://s3.amazonaws.com/media-p.slid.es/uploads/ericmoura/images/1227000/Passive_Reinforcement_Learning.png)\n",
        "\n",
        "## actif\n",
        "![Texte alternatif…](https://s3.amazonaws.com/media-p.slid.es/uploads/ericmoura/images/1227002/Active_Reinforcement_Learning_1.png)\n",
        "![Texte alternatif…](https://s3.amazonaws.com/media-p.slid.es/uploads/ericmoura/images/1227004/Active_Reinforcement_Learning_2.png)\n",
        "![Texte alternatif…](https://s3.amazonaws.com/media-p.slid.es/uploads/ericmoura/images/1227005/Active_Reinforcement_Learning_3.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "F1-pMu5teCbE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "--- \n",
        "\n",
        "\n",
        "# Les algos :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Texte alternatif…](https://pbs.twimg.com/media/DyfDnBnWsAAJ456.jpg:large)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* ** SARSA** (State-Action-Reward-State-Action)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*  **MOnte Carlo / TD ?**\n",
        "![Texte alternatif…](https://cdn-images-1.medium.com/max/1000/1*LLfj11fivpkKZkwQ8uPi3A.png)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*  **Q-learning**\n",
        "\n",
        "![Texte alternatif…](https://randomant.net/images/algorithm-behind-curtain-3/q_learning_algorithm_1.gif)\n",
        "![Texte alternatif…](https://randomant.net/images/algorithm-behind-curtain-3/q_learning_algorithm_2.gif)\n",
        "![Texte alternatif…](https://cdancette.fr/assets/q-formula.svg)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*  **Deep Q-Networks**(DQNs) which use Neural Networks to estimate Q-values.\n",
        "*L’idée principale était d’utiliser des réseaux de neurones profonds pour représenter le Q-réseau, et de former ce Q-réseau pour prédire la récompense totale. *\n",
        "![Texte alternatif…](https://cdn-images-1.medium.com/max/800/1*w5GuxedZ9ivRYqM_MLUxOQ.png)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*  **A3C** (algorithme acteur-critique asynchrone)  \n",
        "*A3C , combine un DQN avec un réseau pour la sélection des actions.* \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **ARS** (Augmented Random Search)\n",
        "http://penseeartificielle.fr/focus-augmented-random-search-ars-tutoriel/\n",
        "\n",
        "cf source : https://ludo-louis.fr/futur-deep-learning-reinforcement-learning/\n",
        "cf source :  https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419\n",
        "cf source : http://www-igm.univ-mlv.fr/~dr/XPOSE2014/Machin_Learning/index.html\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bSM1HDCbGAAQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qZT4Xoa-wwj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqwo1uJY-xDn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Les Tutos\n",
        "(autre : https://github.com/dennybritz/reinforcement-learning)\n",
        "\n",
        "exemple TicTacToe\n",
        "\n",
        "-> Q-learning \n",
        "-> minimax\n",
        "source : https://sandipanweb.wordpress.com/2017/03/30/using-minimax-without-pruning-to-implement-the-machine-players-to-play-tictactoe-in-computer/\n",
        "\n",
        "---\n",
        "\n",
        "exemple Grid World\n",
        "\n",
        "-> Q-Learning\n",
        "-> DQN\n",
        "\n",
        "source : https://cdancette.fr/2017/08/18/reinforcement-learning-part1/\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "exempe PacMan\n",
        "\n",
        "source OpenAI gym : https://gym.openai.com/\n",
        "\n",
        "-> DQN\n",
        "\n",
        "--- \n",
        "\n",
        "exemple pybullet\n",
        "\n",
        "source : http://penseeartificielle.fr/focus-augmented-random-search-ars-tutoriel/\n",
        "\n",
        "-> ARS"
      ]
    },
    {
      "metadata": {
        "id": "KafW-DtViUOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}